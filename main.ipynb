{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "    Notice that breakout-ram is hard because the state is 128 bytes from the ram\n",
    "    and the bytes do not have an intuitive meaning\n",
    "    also notice that 50M samples is typical for DQNs with visual input (refer to rainbow)\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FrameStack\n",
    "\n",
    "class BreakoutWrapper(gym.ObservationWrapper):\n",
    "    \"\"\" \n",
    "    takes (210, 160, 3) to (40, 40)\n",
    "    stops training when one life is lost\n",
    "    converts to grey scale float\n",
    "    cuts the margins\n",
    "    \n",
    "    fires the ball by pressing action 1\n",
    "    \n",
    "    wrapped by framestack (not wrapping FrameStack) to utilize lazy frame for memory saving\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.observation_space = gym.spaces.Box(0, 1, (170, 160))\n",
    "        self.pooling = torch.nn.AvgPool2d(kernel_size=(4,4), stride=(4, 4))\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        lives = self.env.unwrapped.ale.lives()\n",
    "        if lives < self.lives:\n",
    "            done = True\n",
    "        self.lives = lives\n",
    "        return self.observation(obs), reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        self.lives = self.env.unwrapped.ale.lives()\n",
    "        obs, _, _, _ = self.step(1)\n",
    "        return obs\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.array(observation).astype(np.float32) / 255.0\n",
    "        observation = observation[30:-17] \n",
    "        observation = np.mean(observation, axis=2) # greyscale\n",
    "        tmp = torch.as_tensor(observation).unsqueeze(0)\n",
    "        observation = np.array(self.pooling(tmp).squeeze(0))\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "env_name = 'Breakout-v0'\n",
    "env_fn = lambda: FrameStack(BreakoutWrapper(gym.make(env_name)), 4)\n",
    "\n",
    "env = env_fn()\n",
    "result  = np.array(env.reset())\n",
    "result = np.array(result).transpose(1, 2, 0) # 0 is white\n",
    "#plt.imshow(result[:, :, -1], cmap='Greys') \n",
    "plt.imshow(1-result[:, :, 1:4]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartpoleWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "\n",
    "    def observation(self, x):\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        return x\n",
    "    \n",
    "env_name = 'CartPole-v1'\n",
    "env_fn = lambda: CartpoleWrapper(gym.make(env_name))\n",
    "\n",
    "env = env_fn()\n",
    "result  = np.array(env.reset())\n",
    "print(result, result.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Agent\n",
    "    QLearning\n",
    "    SAC-discrete, tested on CartPole, run on Breakout\n",
    "    MBPO\n",
    "\n",
    "In general, when an algo does not work, try large batch low lr with few updates\n",
    "\n",
    "It is okay if the loss of Q increases significantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Config\n",
    "from models import MLP\n",
    "from agents import MBPO\n",
    "\"\"\"\n",
    "    the hyperparameters are the same as MBPO\n",
    "\"\"\"\n",
    "algo_args = Config()\n",
    "\n",
    "algo_args.n_warmup=int(5e3)\n",
    "\"\"\"\n",
    " rainbow said 2e5 samples or 5e4 updates is typical for Qlearning\n",
    " bs256lr3e-4, it takes 2e4updates\n",
    " for the model on CartPole to learn done...\n",
    "\n",
    " Only 3e5 samples are needed for parameterized input continous motion control\n",
    "\"\"\"\n",
    "algo_args.replay_size=int(1e6)\n",
    "algo_args.max_ep_len=500\n",
    "algo_args.test_interval = int(1e4)\n",
    "algo_args.seed=0\n",
    "algo_args.batch_size=256 # the same as MBPO\n",
    "algo_args.save_interval=600 # in seconds\n",
    "algo_args.log_interval=int(2e3/200)\n",
    "algo_args.n_step=int(1e8)\n",
    "\n",
    "p_args=Config()\n",
    "p_args.network = MLP\n",
    "p_args.activation=torch.nn.ReLU\n",
    "p_args.lr=3e-4\n",
    "p_args.sizes = [4, 16, 32, 3] \n",
    "p_args.update_interval=1/10\n",
    "\"\"\"\n",
    " bs=32 interval=4 from rainbow Q\n",
    " MBPO retrains fram scratch periodically\n",
    " in principle this can be arbitrarily frequent\n",
    "\"\"\"\n",
    "p_args.n_p=7 # ensemble\n",
    "p_args.refresh_interval=int(1e3) # refreshes the model buffer\n",
    "# ideally rollouts should be used only once\n",
    "p_args.branch=400\n",
    "p_args.roll_length=1 # length > 1 not implemented yet\n",
    "\n",
    "q_args=Config()\n",
    "q_args.network = MLP\n",
    "q_args.activation=torch.nn.ReLU\n",
    "q_args.lr=3e-4\n",
    "q_args.sizes = [4, 16, 32, 3] # 2 actions, dueling q learning\n",
    "q_args.update_interval=1/20\n",
    "# MBPO used 1/40 for continous control tasks\n",
    "# 1/20 for invert pendulum\n",
    "\n",
    "pi_args=Config()\n",
    "pi_args.network = MLP\n",
    "pi_args.activation=torch.nn.ReLU\n",
    "pi_args.lr=3e-4\n",
    "pi_args.sizes = [4, 16, 32, 2] \n",
    "pi_args.update_interval=1/20\n",
    "\n",
    "agent_args=Config()\n",
    "agent_args.agent=MBPO\n",
    "agent_args.gamma=0.99\n",
    "agent_args.alpha=0.2 \n",
    "agent_args.target_sync_rate=5e-3\n",
    "# called tau in MBPO\n",
    "# sync rate per update = update interval/target sync interval\n",
    "\n",
    "args = Config()\n",
    "args.env_name=env_name\n",
    "args.name=f\"{args.env_name}_{agent_args.agent}\"\n",
    "device = 0\n",
    "\n",
    "q_args.env_fn = env_fn\n",
    "agent_args.env_fn = env_fn\n",
    "algo_args.env_fn = env_fn\n",
    "\n",
    "agent_args.p_args = p_args\n",
    "agent_args.q_args = q_args\n",
    "agent_args.pi_args = pi_args\n",
    "algo_args.agent_args = agent_args\n",
    "args.algo_args = algo_args # do not call toDict() before config is set\n",
    "\n",
    "print(f\"rollout reuse:{(p_args.refresh_interval/q_args.update_interval*algo_args.batch_size)/algo_args.replay_size}\")\n",
    "# each generated data will be used so many times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 44/1000000 [00:55<814:18:32,  2.93s/it]\u001b[A\n",
      "  0%|          | 45/1000000 [00:58<821:40:51,  2.96s/it]\u001b[A\n",
      "  0%|          | 46/1000000 [01:01<827:32:41,  2.98s/it]\u001b[A\n",
      "  0%|          | 47/1000000 [01:04<834:00:36,  3.00s/it]\u001b[A\n",
      "  0%|          | 48/1000000 [01:07<837:44:33,  3.02s/it]\u001b[A\n",
      "  0%|          | 49/1000000 [01:10<840:28:17,  3.03s/it]\u001b[A\n",
      "  0%|          | 50/1000000 [01:13<842:28:30,  3.03s/it]\u001b[A\n",
      "  0%|          | 51/1000000 [01:16<844:02:53,  3.04s/it]\u001b[A\n",
      "  0%|          | 52/1000000 [01:20<844:58:56,  3.04s/it]\u001b[A\n",
      "  0%|          | 53/1000000 [01:23<850:17:16,  3.06s/it]\u001b[A\n",
      "  0%|          | 54/1000000 [01:26<850:26:32,  3.06s/it]\u001b[A\n",
      "  0%|          | 55/1000000 [01:29<851:05:40,  3.06s/it]\u001b[A\n",
      "  0%|          | 56/1000000 [01:32<849:46:09,  3.06s/it]\u001b[A\n",
      "  0%|          | 57/1000000 [01:35<846:54:39,  3.05s/it]\u001b[A\n",
      "  0%|          | 58/1000000 [01:38<843:41:08,  3.04s/it]\u001b[A\n",
      "  0%|          | 59/1000000 [01:41<840:52:29,  3.03s/it]\u001b[A\n",
      "  0%|          | 60/1000000 [01:44<840:11:34,  3.02s/it]\u001b[A\n",
      "  0%|          | 61/1000000 [01:47<837:35:23,  3.02s/it]\u001b[A\n",
      "  0%|          | 62/1000000 [01:50<837:34:52,  3.02s/it]\u001b[A\n",
      "  0%|          | 63/1000000 [01:53<835:45:23,  3.01s/it]\u001b[A\n",
      "  0%|          | 64/1000000 [01:56<837:09:17,  3.01s/it]\u001b[A\n",
      "  0%|          | 65/1000000 [01:59<839:03:00,  3.02s/it]\u001b[A\n",
      "  0%|          | 66/1000000 [02:02<840:27:09,  3.03s/it]\u001b[A\n",
      "  0%|          | 67/1000000 [02:05<842:20:42,  3.03s/it]\u001b[A\n",
      "  0%|          | 68/1000000 [02:08<842:01:19,  3.03s/it]\u001b[A\n",
      "  0%|          | 69/1000000 [02:11<839:21:35,  3.02s/it]\u001b[A\n",
      "  0%|          | 70/1000000 [02:14<838:36:01,  3.02s/it]\u001b[A\n",
      "  0%|          | 71/1000000 [02:17<835:38:18,  3.01s/it]\u001b[A\n",
      "  0%|          | 72/1000000 [02:20<835:37:07,  3.01s/it]\u001b[A\n",
      "  0%|          | 73/1000000 [02:23<836:26:07,  3.01s/it]\u001b[A\n",
      "  0%|          | 74/1000000 [02:26<835:26:48,  3.01s/it]\u001b[A\n",
      "  0%|          | 75/1000000 [02:29<836:25:12,  3.01s/it]\u001b[A\n",
      "  0%|          | 76/1000000 [02:32<835:20:59,  3.01s/it]\u001b[A\n",
      "  0%|          | 77/1000000 [02:35<834:50:53,  3.01s/it]\u001b[A\n",
      "  0%|          | 78/1000000 [02:38<837:30:52,  3.02s/it]\u001b[A\n",
      "  0%|          | 79/1000000 [02:41<795:37:30,  2.86s/it]\u001b[A\n",
      "  0%|          | 80/1000000 [02:44<813:50:35,  2.93s/it]\u001b[A\n",
      "  0%|          | 81/1000000 [02:47<824:01:50,  2.97s/it]\u001b[A\n",
      "  0%|          | 82/1000000 [02:50<827:04:45,  2.98s/it]\u001b[A\n",
      "  0%|          | 83/1000000 [02:53<830:21:54,  2.99s/it]\u001b[A\n",
      "  0%|          | 84/1000000 [02:56<831:29:01,  2.99s/it]\u001b[A\n",
      "  0%|          | 85/1000000 [02:59<836:27:19,  3.01s/it]\u001b[A\n",
      "  0%|          | 86/1000000 [03:02<841:20:04,  3.03s/it]\u001b[A\n",
      "  0%|          | 87/1000000 [03:05<860:31:22,  3.10s/it]\u001b[A\n",
      "  0%|          | 88/1000000 [03:08<874:27:55,  3.15s/it]\u001b[A\n",
      "  0%|          | 89/1000000 [03:12<871:46:22,  3.14s/it]\u001b[A\n",
      "  0%|          | 90/1000000 [03:15<863:02:04,  3.11s/it]\u001b[A\n",
      "  0%|          | 91/1000000 [03:18<858:22:05,  3.09s/it]\u001b[A\n",
      "  0%|          | 92/1000000 [03:21<855:12:50,  3.08s/it]\u001b[A\n",
      "  0%|          | 93/1000000 [03:24<852:49:45,  3.07s/it]\u001b[A\n",
      "  0%|          | 94/1000000 [03:27<850:38:02,  3.06s/it]\u001b[A\n",
      "  0%|          | 95/1000000 [03:30<849:37:36,  3.06s/it]\u001b[A\n",
      "  0%|          | 96/1000000 [03:33<848:54:40,  3.06s/it]\u001b[A\n",
      "  0%|          | 97/1000000 [03:36<851:33:45,  3.07s/it]\u001b[A\n",
      "  0%|          | 98/1000000 [03:39<851:22:23,  3.07s/it]\u001b[A\n",
      "  0%|          | 99/1000000 [03:42<850:45:31,  3.06s/it]\u001b[A\n",
      "  0%|          | 100/1000000 [03:45<850:20:08,  3.06s/it]\u001b[A\n",
      "  0%|          | 101/1000000 [03:48<850:15:17,  3.06s/it]\u001b[A\n",
      "  0%|          | 102/1000000 [03:51<850:26:55,  3.06s/it]\u001b[A\n",
      "  0%|          | 103/1000000 [03:54<849:24:10,  3.06s/it]\u001b[A\n",
      "  0%|          | 104/1000000 [03:57<848:35:38,  3.06s/it]\u001b[A\n",
      "  0%|          | 105/1000000 [04:00<847:17:28,  3.05s/it]\u001b[A\n",
      "  0%|          | 106/1000000 [04:03<847:23:35,  3.05s/it]\u001b[A\n",
      "  0%|          | 107/1000000 [04:07<847:43:15,  3.05s/it]\u001b[A\n",
      "  0%|          | 108/1000000 [04:10<847:40:43,  3.05s/it]\u001b[A\n",
      "  0%|          | 109/1000000 [04:13<847:33:26,  3.05s/it]\u001b[A\n",
      "  0%|          | 110/1000000 [04:16<847:20:44,  3.05s/it]\u001b[A\n",
      "  0%|          | 111/1000000 [04:19<847:12:55,  3.05s/it]\u001b[A\n",
      "  0%|          | 112/1000000 [04:22<846:15:47,  3.05s/it]\u001b[A\n",
      "  0%|          | 113/1000000 [04:25<846:29:45,  3.05s/it]\u001b[A\n",
      "  0%|          | 114/1000000 [04:28<847:05:33,  3.05s/it]\u001b[A\n",
      "  0%|          | 115/1000000 [04:31<846:50:12,  3.05s/it]\u001b[A\n",
      "  0%|          | 116/1000000 [04:34<846:51:53,  3.05s/it]\u001b[A\n",
      "  0%|          | 117/1000000 [04:37<846:53:07,  3.05s/it]\u001b[A\n",
      "  0%|          | 118/1000000 [04:40<847:06:05,  3.05s/it]\u001b[A\n",
      "  0%|          | 119/1000000 [04:43<847:16:09,  3.05s/it]\u001b[A\n",
      "  0%|          | 120/1000000 [04:46<847:17:23,  3.05s/it]\u001b[A\n",
      "  0%|          | 121/1000000 [04:49<847:47:19,  3.05s/it]\u001b[A\n",
      "  0%|          | 122/1000000 [04:52<853:27:31,  3.07s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from algorithm import RL\n",
    "from utils import Logger\n",
    "\n",
    "RL(logger = Logger(args), device=device, **algo_args._toDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.as_tensor(state,  dtype=torch.float).to(device)\n",
    "    action = model.act(tmp, deterministic=False)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.reset()[-1]) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = input()\n",
    "    if len(tmp) == 0:\n",
    "        tmp = \"0\"\n",
    "    action = int(tmp)\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    img.set_data(state[-1]) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "\n",
    "    print(f\"this: {reward}, total: {total}\")\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, **kwargs):\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(**{'x':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
