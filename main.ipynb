{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb\n",
    "from spinup import models\n",
    "core = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "from spinup.algos.ppo.ppo import ppo\n",
    "from spinup.algos.sac.sac import sac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "    Notice that breakout-ram is harder than visual input because the state is 128 bytes from the ram\n",
    "    and the bytes do not have an intuitive meaning\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"Breakout-ram-v0\" # continous action\n",
    "#env_name=\"Breakout-ram-v0\" # discrete action\n",
    "#env_name=\"CartPole-v1\" #discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "    DQN, tested on CartPole\n",
    "    PPO, tested on Breakout\n",
    "    SAC-continous, run on Walker, alpha = 0.2 is too large, 0.05 still large\n",
    "    SAC-discrete, tested on CartPole, run on Breakout\n",
    "        1. must use eps to prevent nan because probablity for some action becomes 0, this happens when Q is large (e.g. a few hundred)\n",
    "        2. If n_update or lr is much too high, entropy may collapse for a reasonable alpha, with a stable high policy regret\n",
    "        3. Sometimes Q stays much lower than ground truth, and learns very slow. I believe there is a trick I need to incorporate\n",
    "        4. when alpha is too large, maximum entropy, test significantly superior than train. when too small, zero entropy. \n",
    "        A heuristic: alpha leads to an additional reward about alpha*entropy, which should be smaller than the reward per step.\n",
    "        5. on Breakout, 3 happens for Q, pi learns never firing the ball!? randomly picking an action from the other three...\n",
    "\n",
    "In general, when an algo does not work, try large batch low lr with few updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=\"Breakout-ram-v0\" #discrete action\n",
    "args.algorithm=\"dqn\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=2000\n",
    "algo_args.n_update=100\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=5000\n",
    "algo_args.update_after=5000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.pi_lr=1e-5\n",
    "model_args.q_lr=1e-4\n",
    "model_args.alpha=0\n",
    "model_args.eps=1e-2\n",
    "model_args.dqn=True\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=\"Breakout-ram-v0\" #discrete action\n",
    "args.algorithm=\"sac\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=500\n",
    "algo_args.n_update=30\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=20000\n",
    "algo_args.update_after=20000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.pi_lr=1e-5\n",
    "model_args.q_lr=1e-4\n",
    "model_args.alpha=0.1\n",
    "model_args.eps=1e-5\n",
    "model_args.dqn=False\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"ppo\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "args.steps_per_epoch=5000\n",
    "args.epochs=500\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0\n",
    "model_args.eps=0.01\n",
    "model_args.dqn=True\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "\n",
    "#mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "result = ppo(lambda : gym.make(args.env), actor_critic=core.MLPVActorCritic,\n",
    "    ac_kwargs=dict(hidden_sizes=(args.hid,)*args.l, logger=logger,  gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps_per_epoch, epochs=args.epochs)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "state, reward, done, info  = env.step(1)\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.as_tensor(state,  dtype=torch.float).to(device)\n",
    "    action = model.act(tmp, deterministic=False)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "state, reward, done, info  = env.step(1)\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = input()\n",
    "    if len(tmp) == 0:\n",
    "        tmp = \"0\"\n",
    "    action = int(tmp)\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    print(f\"this: {reward}, total: {total}\")\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 4,\n",
       " 'shape': (),\n",
       " 'dtype': dtype('int64'),\n",
       " 'np_random': RandomState(MT19937) at 0x7FCA30899990}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn' has no attribute 'Convolution'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-2806a71ec3aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Convolution'"
     ]
    }
   ],
   "source": [
    "torch.nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
