{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb\n",
    "from spinup import models\n",
    "core = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "from spinup.algos.ppo.ppo import ppo\n",
    "from spinup.algos.sac.sac import sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def toDict(self):\n",
    "        pr = {}\n",
    "        for name in dir(self):\n",
    "            value = getattr(self, name)\n",
    "            if not name.startswith('__') and not callable(value) and not name.endswith('_'):\n",
    "                pr[name] = value\n",
    "        return pr\n",
    "    \n",
    "class TabularLogger(object):\n",
    "    \"\"\"\n",
    "    A text interface logger, outputs mean and std several times per epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        \n",
    "    def log(dic, commit=False):\n",
    "        if commit:\n",
    "            print\n",
    "        \n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    A logger wrapper for visualized loggers, such as tb or wandb\n",
    "    Automatically counts steps, epoch, etc. and sets logging interval\n",
    "    to prevent the log becoming too big\n",
    "    uses kwargs instead of dict for convenience\n",
    "    all None valued keys are counters\n",
    "    \"\"\"\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.counters = {'epoch':0}\n",
    "        self.frequency = 10 # logs per epoch\n",
    "        \n",
    "    def log(self, data=None, **kwargs):\n",
    "        if data is None:\n",
    "            data = {}\n",
    "        data.update(kwargs)\n",
    "        # counting\n",
    "        for key in data:\n",
    "            if not key in self.counters:\n",
    "                self.counters[key] = 0\n",
    "            self.counters[key] += 1\n",
    "            \n",
    "        to_store = {}\n",
    "        epoch = self.counters['epoch']\n",
    "        for key in data:\n",
    "            count = self.counters[key]\n",
    "            period = count//(epoch+1) + 1\n",
    "            flag = random.random()< self.frequency/period\n",
    "            if flag:\n",
    "                if data[key] is None:\n",
    "                    to_store[key] = self.counters[key]\n",
    "                else:\n",
    "                    valid = True\n",
    "                    if isinstance(data[key], torch.Tensor):\n",
    "                        data[key] = data[key].detach().cpu()\n",
    "                        if  torch.isnan(data[key]).any():\n",
    "                            valid = False\n",
    "                    elif np.isnan(data[key]).any():\n",
    "                        valid = False\n",
    "                    if not valid:\n",
    "                        print(f'{key} is nan!')\n",
    "                        continue\n",
    "                    to_store[key] = data[key]\n",
    "                \n",
    "        if len(to_store) > 0:\n",
    "            self.logger.log(to_store, commit=True)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.logger.log(data={'epoch':self.counters['epoch']}, commit=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "the agent need to press the \"fire\" button or it gets stuck forever\n",
    "supporting both parameterized state or visual input\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"Breakout-ram-v0\" # continous action\n",
    "#env_name=\"Breakout-ram-v0\" # discrete action\n",
    "#env_name=\"CartPole-v1\" #discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "    DQN, tested on CartPole\n",
    "    PPO, tested on Breakout\n",
    "    SAC-continous, run on Walker, alpha = 0.2 is too large, 0.05 still large\n",
    "    SAC-discrete, tested on CartPole, run on Breakout\n",
    "        1. must use eps to prevent nan because probablity for some action becomes 0, this happens when Q is large (e.g. a few hundred)\n",
    "        2. If n_update or lr is much too high, entropy may collapse for a reasonable alpha, with a stable high policy regret\n",
    "        3. Sometimes Q stays much lower than ground truth, and learns very slow. I believe there is a trick I need to incorporate\n",
    "        4. when alpha is too large, maximum entropy, test significantly superior than train. when too small, zero entropy. \n",
    "        A heuristic: alpha leads to an additional reward about alpha*entropy, which should be smaller than the reward per step.\n",
    "        5. on Breakout, it learns never firing the ball!? randomly picking an action from the other three...\n",
    "\n",
    "In general, when an algo does not work, try large batch low lr with few updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hk2cdfl1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 20850<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:hk2cdfl1). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.29 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Breakout-ram-v0_dqn</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/henry_b/RL\" target=\"_blank\">https://wandb.ai/henry_b/RL</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/henry_b/RL/runs/1a25hso0\" target=\"_blank\">https://wandb.ai/henry_b/RL/runs/1a25hso0</a><br/>\n",
       "                Run data is saved locally in <code>/home/asus/spinningup/wandb/run-20210505_110816-1a25hso0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of actions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/9999 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of parameters: \t pi: 99844, \t q1: 99844, \t q2: 99844\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <generator object tqdm.__iter__ at 0x7fca302270c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/tqdm/std.py\", line 1193, in __iter__\n",
      "    self.close()\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/tqdm/std.py\", line 1287, in close\n",
      "    fp_write('')\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/tqdm/std.py\", line 1284, in fp_write\n",
      "    self.fp.write(_unicode(s))\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/tqdm/utils.py\", line 142, in inner\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/wandb/sdk/lib/redirect.py\", line 100, in new_write\n",
      "    cb(name, data)\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/wandb/sdk/wandb_run.py\", line 739, in _console_callback\n",
      "    self._backend.interface.publish_output(name, data)\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 186, in publish_output\n",
      "    self._publish_output(o)\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 191, in _publish_output\n",
      "    self._publish(rec)\n",
      "  File \"/home/asus/anaconda3/envs/MARL/lib/python3.7/site-packages/wandb/sdk/interface/interface.py\", line 514, in _publish\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "  0%|          | 3/9999 [06:24<355:32:46, 128.05s/it]\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/9999 [00:17<47:25:13, 17.07s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 2/9999 [00:23<30:19:05, 10.92s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 3/9999 [00:45<44:02:46, 15.86s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 4/9999 [01:03<46:30:39, 16.75s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 5/9999 [01:25<51:36:30, 18.59s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 6/9999 [01:47<54:39:06, 19.69s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 7/9999 [02:09<56:36:42, 20.40s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 8/9999 [02:28<55:27:03, 19.98s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 9/9999 [02:50<57:12:23, 20.62s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 10/9999 [03:12<58:18:22, 21.01s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 11/9999 [03:34<59:23:56, 21.41s/it]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "args = Config()\n",
    "args.env=\"Breakout-ram-v0\" #discrete action\n",
    "args.algorithm=\"dqn\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=2000\n",
    "algo_args.n_update=100\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=5000\n",
    "algo_args.update_after=5000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.pi_lr=1e-5\n",
    "model_args.q_lr=1e-4\n",
    "model_args.alpha=0\n",
    "model_args.eps=1e-2\n",
    "model_args.dqn=True\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=\"Breakout-ram-v0\" #discrete action\n",
    "args.algorithm=\"sac\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=500\n",
    "algo_args.n_update=30\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=20000\n",
    "algo_args.update_after=20000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.pi_lr=1e-5\n",
    "model_args.q_lr=1e-4\n",
    "model_args.alpha=0.1\n",
    "model_args.eps=1e-5\n",
    "model_args.dqn=False\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"ppo\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "args.steps_per_epoch=5000\n",
    "args.epochs=500\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0\n",
    "model_args.eps=0.01\n",
    "model_args.dqn=True\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "\n",
    "#mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "result = ppo(lambda : gym.make(args.env), actor_critic=core.MLPVActorCritic,\n",
    "    ac_kwargs=dict(hidden_sizes=(args.hid,)*args.l, logger=logger,  gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps_per_epoch, epochs=args.epochs)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "state, reward, done, info  = env.step(1)\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.as_tensor(state,  dtype=torch.float).to(device)\n",
    "    action = model.act(tmp, deterministic=False)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "state, reward, done, info  = env.step(1)\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = input()\n",
    "    if len(tmp) == 0:\n",
    "        tmp = \"0\"\n",
    "    action = int(tmp)\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    print(f\"this: {reward}, total: {total}\")\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
