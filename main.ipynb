{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "    Notice that breakout-ram is harder than visual input because the state is 128 bytes from the ram\n",
    "    and the bytes do not have an intuitive meaning\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"Breakout-ram-v0\" # continous action\n",
    "#env_name=\"Breakout-ram-v0\" # discrete action\n",
    "#env_name=\"CartPole-v1\" #discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "    DQN, tested on CartPole\n",
    "    PPO, tested on Breakout\n",
    "    SAC-continous, run on Walker, alpha = 0.2 is too large, 0.05 still large\n",
    "    SAC-discrete, tested on CartPole, run on Breakout\n",
    "        1. must use eps to prevent nan because probablity for some action becomes 0, this happens when Q is large (e.g. a few hundred)\n",
    "        2. If n_update or lr is much too high, entropy may collapse for a reasonable alpha, with a stable high policy regret\n",
    "        3. Sometimes Q stays much lower than ground truth, and learns very slow. I believe there is a trick I need to incorporate\n",
    "        4. when alpha is too large, maximum entropy, test significantly superior than train. when too small, zero entropy. \n",
    "        A heuristic: alpha leads to an additional reward about alpha*entropy, which should be smaller than the reward per step.\n",
    "        5. on Breakout, 3 happens for Q, pi learns never firing the ball!? randomly picking an action from the other three...\n",
    "\n",
    "In general, when an algo does not work, try large batch low lr with few updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Config\n",
    "from models import CNN\n",
    "from agents import QLearning\n",
    "\n",
    "args = Config()\n",
    "args.agent = QLearning\n",
    "args.env_name=\"Breakout-v0\"\n",
    "args.name=f\"{args.env_name}_{args.agent}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=2000\n",
    "algo_args.n_update=100\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=5000\n",
    "algo_args.update_after=5000\n",
    "\n",
    "agent_args=Config()\n",
    "agent_args.gamma=0.99\n",
    "agent_args.eps=3e-2\n",
    "agent_args.target_sync_rate=5e-3\n",
    "\n",
    "q_args=Config()\n",
    "q_args.network = CNN\n",
    "q_args.hidden_sizes=[256]*4\n",
    "q_args.activation=torch.nn.ReLU\n",
    "q_args.lr=1e-4\n",
    "q_args.strides = [2]*7\n",
    "q_args.kernels = [3]*7\n",
    "q_args.paddings = [1]*7\n",
    "q_args.sizes = [4, 16, 16, 32, 64, 128, 128, 4]\n",
    "\n",
    "agent_args.q_args = q_args._toDict()\n",
    "algo_args.agent_args = agent_args._toDict()\n",
    "args.algo_args = algo_args._toDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import FrameStack\n",
    "\n",
    "class BreakoutWrapper(gym.ObservationWrapper):\n",
    "    \"\"\" \n",
    "    takes (n_frames, 210, 160, 3) \n",
    "    converts to grey scale float\n",
    "    cuts the margins\n",
    "    \n",
    "    fires the ball by pressing action 1\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        env = FrameStack(env, 4)\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.env.reset(**kwargs)\n",
    "        obs, _, _, _ = self.step(1)\n",
    "        return obs\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = np.array(observation).astype(np.float32) / 255.0\n",
    "        observation = observation[:, 25:-15] # to 170, 160\n",
    "        observation = np.mean(observation, axis=3) # greyscale\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from agents import QLearning\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "agent_args.env_fn = lambda: BreakoutWrapper(gym.make(args.env_name))\n",
    "env = agent_args.env_fn()\n",
    "result  = np.array(env.reset())\n",
    "plt.imshow(result[-1], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Logger\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env_name,\n",
    ")\n",
    "logger = Logger(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net = CNN\n",
    "agent = QLearning(logger=logger, **agent_args._toDict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_args.toDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithm import RL\n",
    "\n",
    "\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=\"Breakout-ram-v0\" #discrete action\n",
    "args.algorithm=\"sac\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=1\n",
    "args.seed=0\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.max_ep_len=500\n",
    "algo_args.n_update=30\n",
    "algo_args.batch_size=512\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=20000\n",
    "algo_args.update_after=20000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.pi_lr=1e-5\n",
    "model_args.q_lr=1e-4\n",
    "model_args.alpha=0.1\n",
    "model_args.eps=1e-5\n",
    "model_args.dqn=False\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=args.gpu\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"ppo\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "args.steps_per_epoch=5000\n",
    "args.epochs=500\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0\n",
    "model_args.eps=0.01\n",
    "model_args.dqn=True\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "\n",
    "#mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "result = ppo(lambda : gym.make(args.env), actor_critic=core.MLPVActorCritic,\n",
    "    ac_kwargs=dict(hidden_sizes=(args.hid,)*args.l, logger=logger,  gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps_per_epoch, epochs=args.epochs)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.as_tensor(state,  dtype=torch.float).to(device)\n",
    "    action = model.act(tmp, deterministic=False)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img = plt.imshow(env.reset()[-1]) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = input()\n",
    "    if len(tmp) == 0:\n",
    "        tmp = \"0\"\n",
    "    action = int(tmp)\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    img.set_data(state[-1]) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    \n",
    "\n",
    "    print(f\"this: {reward}, total: {total}\")\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Conv2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
