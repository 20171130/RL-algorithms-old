{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pdb\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import gym\n",
    "import time\n",
    "import wandb\n",
    "from spinup import models\n",
    "core = models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "from spinup.algos.ppo.ppo import ppo\n",
    "from spinup.algos.sac.sac import sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        return None\n",
    "    def toDict(self):\n",
    "        pr = {}\n",
    "        for name in dir(self):\n",
    "            value = getattr(self, name)\n",
    "            if not name.startswith('__') and not callable(value) and not name.endswith('_'):\n",
    "                pr[name] = value\n",
    "        return pr\n",
    "    \n",
    "class TabularLogger(object):\n",
    "    \"\"\"\n",
    "    A text interface logger, outputs mean and std several times per epoch\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.buffer = {}\n",
    "        \n",
    "    def log(dic, commit=False):\n",
    "        if commit:\n",
    "            print\n",
    "        \n",
    "class Logger(object):\n",
    "    \"\"\"\n",
    "    A logger wrapper for visualized loggers, such as tb or wandb\n",
    "    Automatically counts steps, epoch, etc. and sets logging interval\n",
    "    to prevent the log becoming too big\n",
    "    uses kwargs instead of dict for convenience\n",
    "    all None valued keys are counters\n",
    "    \"\"\"\n",
    "    def __init__(self, logger):\n",
    "        self.logger = logger\n",
    "        self.counters = {'epoch':0}\n",
    "        self.frequency = 10 # logs per epoch\n",
    "        \n",
    "    def log(self, data=None, **kwargs):\n",
    "        if data is None:\n",
    "            data = {}\n",
    "        data.update(kwargs)\n",
    "        # counting\n",
    "        for key in data:\n",
    "            if not key in self.counters:\n",
    "                self.counters[key] = 0\n",
    "            self.counters[key] += 1\n",
    "            \n",
    "        to_store = {}\n",
    "        epoch = self.counters['epoch']\n",
    "        for key in data:\n",
    "            count = self.counters[key]\n",
    "            period = count//(epoch+1) + 1\n",
    "            flag = random.random()< self.frequency/period\n",
    "            if flag:\n",
    "                if data[key] is None:\n",
    "                    to_store[key] = self.counters[key]\n",
    "                else:\n",
    "                    valid = True\n",
    "                    if isinstance(data[key], torch.Tensor):\n",
    "                        data[key] = data[key].detach().cpu()\n",
    "                        if  torch.isnan(data[key]).any():\n",
    "                            valid = False\n",
    "                    elif np.isnan(data[key]).any():\n",
    "                        valid = False\n",
    "                    if not valid:\n",
    "                        print(f'{key} is nan!')\n",
    "                        continue\n",
    "                    to_store[key] = data[key]\n",
    "                \n",
    "        if len(to_store) > 0:\n",
    "            self.logger.log(to_store, commit=True)\n",
    "        \n",
    "    def flush(self):\n",
    "        self.logger.log(data={'epoch':self.counters['epoch']}, commit=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick an Environment\n",
    "CartPole, the MNIST of RL, discrete action, parameterized state\n",
    "\n",
    "Breakout, discrete action, 0 fire, 1 stay, 2 right, 3 left\n",
    "the agent need to press the \"fire\" button or it gets stuck forever\n",
    "supporting both parameterized state or visual input\n",
    "\n",
    "MountainCar, discrete or continous action, parameterized state\n",
    "Gets reward for climbing up a hill that costs energy,\n",
    "painful exploration is essential\n",
    "\n",
    "Walker, peanlty -100 for falling. \n",
    "The initial greedy strategy may make the agent stand unmoved and prevent falling\n",
    "As a result, intial test reward =0 while initial train reward=100\n",
    "\n",
    "For environments with a large penalty, we should use a large batch when updating Q, in order to compensate the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name=\"BipedalWalker-v3\" # continous action\n",
    "#env_name=\"Breakout-ram-v0\" # discrete action\n",
    "#env_name=\"CartPole-v1\" #discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "    DQN, tested on CartPole\n",
    "    PPO, tested on Breakout\n",
    "    SAC-continous, tested on Walker, alpha = 0.2 is too large, 0.05 still large\n",
    "    SAC-discrete, tested on CartPole\n",
    "        must use eps to prevent nan because probablity for some action becomes 0\n",
    "        this happens when Q is large (e.g. a few hundred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"dqn\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "args.steps_per_epoch=5000\n",
    "args.epochs=500\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0\n",
    "model_args.eps=0.01\n",
    "model_args.dqn=True\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger, \n",
    "           steps_per_epoch=args.steps_per_epoch, epochs=args.epochs)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"sac\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "\n",
    "algo_args = Config()\n",
    "algo_args.n_step=4096\n",
    "algo_args.n_update=50\n",
    "algo_args.batch_size=2048\n",
    "algo_args.epochs=9999\n",
    "algo_args.start_steps=20000\n",
    "algo_args.update_after=20000\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0.02\n",
    "model_args.eps=0\n",
    "model_args.dqn=False\n",
    "\n",
    "args.algo_args = algo_args.toDict()\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "env = gym.make(args.env)\n",
    "model = core.MLPDQActorCritic(env.observation_space, env.action_space, logger=logger, **(model_args.toDict()))\n",
    "device=0\n",
    "model.to(device)\n",
    "result =sac(lambda : gym.make(args.env), model=model, logger=logger,  device=device, **(algo_args.toDict()))\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config()\n",
    "args.env=env_name #discrete action\n",
    "args.algorithm=\"ppo\"\n",
    "args.name=f\"{args.env}_{args.algorithm}\"\n",
    "args.gpu=0\n",
    "args.seed=0\n",
    "args.cpu=4\n",
    "args.steps_per_epoch=5000\n",
    "args.epochs=500\n",
    "\n",
    "model_args=Config()\n",
    "model_args.hidden_sizes=[256]*4\n",
    "model_args.activation=torch.nn.ReLU\n",
    "model_args = Config()\n",
    "model_args.gamma=0.99\n",
    "model_args.polyak=0.995\n",
    "model_args.lr=3e-5\n",
    "model_args.alpha=0\n",
    "model_args.eps=0.01\n",
    "model_args.dqn=True\n",
    "args.model_args = model_args.toDict()\n",
    "\n",
    "\n",
    "#mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "run=wandb.init(\n",
    "    project=\"RL\",\n",
    "    config=args,\n",
    "    name=args.name,\n",
    "    group=args.env,\n",
    ")\n",
    "logger = Logger(run)\n",
    "result = ppo(lambda : gym.make(args.env), actor_critic=core.MLPVActorCritic,\n",
    "    ac_kwargs=dict(hidden_sizes=(args.hid,)*args.l, logger=logger,  gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps_per_epoch, epochs=args.epochs)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from IPython import display\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "total = 0\n",
    "for _ in range(2000):\n",
    "    img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    tmp = torch.tensor(state).float()\n",
    "    action = model.act(tmp, deterministic=True)\n",
    "   # action = env.action_space.sample()\n",
    "    state, reward, done, info  = env.step(action)\n",
    "    total += reward\n",
    "    if done:\n",
    "        print(f\"episode len {_}, reward {total}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1)\n",
    "x.requires_grad = True\n",
    "y = x+torch.zeros(1)/torch.zeros(1)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.LogSoftmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = iter(tqdm(range(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
